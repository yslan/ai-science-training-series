{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In this notebook we explore the following topics:__\n",
    "\n",
    "- Visualizing Data\n",
    "- Constructing simple Convolutional Neural Networks\n",
    "- Training and Inference\n",
    "- Visualizing/Interpreting trained Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\n",
    "%env HTTPS_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__One hot encoding:__\n",
    "\n",
    "\\begin{align}\n",
    "    0 = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix}, \\hspace{1cm}\n",
    "    1 = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix}, \\hspace{1cm}\n",
    "    2 = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix}, \\hspace{1cm} ..., \\hspace{1cm}\n",
    "    9 = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "[src: [Deep Convolutional-Shepard Interpolation Neural Networks for Image Classification Tasks](https://link.springer.com/chapter/10.1007%2F978-3-319-93000-8_21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualize a sample of Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "images, labels = train_images[:20], train_labels[:20]\n",
    "\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    ax.set_title(str(np.argmax(labels[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### View an Image in More Detail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = np.squeeze(train_images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Define the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/max/800/1*vpCRPR2o3tTl6R3JiRcvyQ.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "[src: [TechTalks gradient descent local minima](https://bdtechtalks.com/2020/04/27/deep-learning-mode-connectivity-adversarial-attacks/gradient-descent-local-minima/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Optimizer__: RMSprop (Root Mean Square Prop) and many other optimizers are \"*different flavors*\" of gradient descent. RMSprop is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\ S_{dW} &= \\beta_{S_{dW}} + (1 - \\beta)(\\frac{\\partial{J}}{\\partial{W}})^2\\\\\n",
    "\\ W &= W - \\alpha \\frac{\\frac{\\partial{J}}{\\partial{W}}}{\\sqrt{S_{dW}}+ \\epsilon}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where: \n",
    "\n",
    "- s  - the exponentially weighted average of past squares of gradients\n",
    "- $\\frac{\\partial{J}}{\\partial{W}}$ - cost gradient with respect to current layer weight tensor\n",
    "- W  - weight tensor\n",
    "- β - hyperparameter to be tuned\n",
    "- α  - the learning rate\n",
    "- ϵ - very small value to avoid dividing by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ReLU ( Rectified Linear Unit )__:  $$f(x) = max(0, x)$$\n",
    "\n",
    "\n",
    "__Some common activation functions__:\n",
    "\n",
    "<img src='images/activations.png' alt=\"a few common activation functions\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "[src: [Introduction to Different Activation Functions for Deep Learning](https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax Function__: \n",
    "\n",
    "\\begin{align}\n",
    "\\ p_i &=  \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}},   \\quad  \\text{for i = 1, ... , k} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Categorical Crossentropy__: Loss function widely used for multiclass classification. It is give by:\n",
    "\n",
    "\\begin{align}\n",
    "\\ L &= - \\sum_i y_i \\log{p_i}\n",
    "\\end{align}\n",
    "\n",
    "where $y_i$ is the true class and $p_i$ is the predicted *probability/score* for the true class.\n",
    "\n",
    "You can read more about Optimizers and Loss Functions [here](http://cs231n.github.io/linear-classify/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Training Loop__:\n",
    "\n",
    "The steps for training/learning from a batch of data are :\n",
    "\n",
    "1. Clear the gradients of all optimized variables\n",
    "2. Forward pass: compute predicted outputs \n",
    "3. Calculate the loss\n",
    "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "5. Update parameters\n",
    "6. Update average training loss\n",
    "\n",
    "Keras wraps up all of these steps in one function: `model.fit()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test the Trained Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we test our trained model on previously unseen test data and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test Accuracy: {:.4f} and Test Loss: {:.4f}'.format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initialize lists to monitor test accuracy\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "pred = model.predict(test_images, verbose=0)\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "labels = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    correct = pred[i] == labels[i]\n",
    "    class_correct[labels[i]] += correct\n",
    "    class_total[labels[i]] += 1\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets see predictions on individual test images. Since `model.predict` expects a batch of images, i.e, a 4-d tensor, we expand the first dimension of the test image before feeding into model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "pred = model.predict(np.expand_dims(test_images[index], axis=0)); pred = np.argmax(pred)\n",
    "plt.imshow(np.squeeze(test_images[index]), cmap='gray')\n",
    "plt.title('Prediction: {}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Define a python generator [More on Generators in cifar-10 notebook]\n",
    "import itertools\n",
    "\n",
    "def generator(images, labels, batch_size):\n",
    "    iterable_1 = iter(images)\n",
    "    iterable_2 = iter(labels)\n",
    "    while True:\n",
    "        l1 = tuple(itertools.islice(iterable_1, 0, batch_size))\n",
    "        l2 = tuple(itertools.islice(iterable_2, 0, batch_size))\n",
    "        if l1 and l2:\n",
    "            yield np.vstack(np.expand_dims(l1, axis=0)), np.vstack(np.expand_dims(l2, axis=0))\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "test_generator = generator(test_images, test_labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(test_generator)\n",
    "labels = np.argmax(labels, axis=-1)\n",
    "\n",
    "# get sample outputs\n",
    "preds = model.predict(images)\n",
    "# convert output probabilities to predicted class\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx]), str(labels[idx])),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of Incorrect Predictions\n",
    "preds = np.argmax(model.predict(test_images), axis=-1)\n",
    "labels = np.argmax(test_labels, axis=-1)\n",
    "incorrect_idxs = np.where( preds != labels )[0]\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for i in np.arange(20):\n",
    "    idx = incorrect_idxs[i]\n",
    "    ax = fig.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(test_images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx]), str(labels[idx])),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualizing second-last-layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-size_restricted.gif\" alt=\"Second Last Layer Activations\" style=\"width: 400px;\"/> (src: 3blue1brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "emb_model = Model(inputs = model.input, outputs = model.get_layer('dense').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "activations = emb_model.predict(test_images, verbose=1)\n",
    "predictions = model.predict(test_images, verbose=1); labels = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Stack 10 examples of activation vectors for each numeral\n",
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(activations[labels==i][:10])\n",
    "seq = np.vstack(seq)\n",
    "\n",
    "\n",
    "# Plot heatmap for the example activations\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(seq, interpolation='nearest', cmap=plt.cm.gnuplot2)\n",
    "plt.yticks(np.arange(0,100, 5), ['--','0s','--','1s','--','2s','--','3s','--','4s','--','5s','--','6s',\n",
    "                                 '--','7s','--','8s','--','9s'])\n",
    "plt.xlabel(\"Second Last Layer's Activations\")\n",
    "plt.ylabel('Numerals')\n",
    "plt.title('Heatmap of activations for each predicted number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with io.open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "    for embed in activations:\n",
    "        out_v.write('\\t'.join([str(x) for x in embed]) + \"\\n\")\n",
    "        \n",
    "with io.open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "    for l in labels:\n",
    "        out_m.write(str(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now go to: https://projector.tensorflow.org/, in the data section click `Load`, and upload the two files: _vecs.tsv_ and _meta.tsv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tSNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/tSNE.png\" alt=\"tSNE\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualizing intermediate-layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = test_images[1]\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "plt.imshow(np.squeeze(img), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This will return a list of 8 Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(first_layer_activation[0, :, :, 6], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(first_layer_activation[0, :, :, 12], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:5]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            #channel_image *= 64\n",
    "            #channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see: [Visualizing and Understanding Convolutional Networks, Zeiler et. al.](https://arxiv.org/pdf/1311.2901.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: [Visualizing intermediate activation in Convolutional Neural Networks with Keras](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualizing heatmap of class activation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To visualize which parts of our image were the most \"prediction\"-like, let's set up the Grad-CAM process.\n",
    "\n",
    "\"Class Activation Map\" (CAM) is a visualization technique which is useful for understanding which parts of a given image led a convnet to its final classification decision, by producing heatmaps of \"class activation\" over input images.\n",
    "\n",
    "You can read more about this technique here: [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization.](https://arxiv.org/abs/1610.02391)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/3744/1*SGPGG7oeSvVlV5sOSQ2iZw.png\" alt=\"MNIST CNN\" style=\"width: 600px;\"/> \n",
    "\n",
    "[src: [MNIST Handwritten Digits Classification using a Convolutional Neural Network (CNN)](https://towardsdatascience.com/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superimposed_img(img, heatmap):\n",
    "    im = np.array(np.squeeze(img) * 255, dtype = np.uint8)\n",
    "    cv2.imwrite('test_img.jpg', im)\n",
    "\n",
    "    # We use cv2 to load the original image\n",
    "    img = cv2.imread('test_img.jpg', 0)\n",
    "\n",
    "    # We resize the heatmap to have the same size as the original image\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # We convert the heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # 0.9 here is a heatmap intensity factor\n",
    "    superimposed_img = heatmap * 0.9 + img\n",
    "\n",
    "    # Save the image to disk\n",
    "    cv2.imwrite('test_img_cam.jpg', superimposed_img)\n",
    "    \n",
    "    cam = cv2.imread('test_img_cam.jpg', 0)\n",
    "    return cam, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_images[2]\n",
    "img = np.expand_dims(img, axis=0)\n",
    "plt.imshow(np.squeeze(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"conv2d_2\" \n",
    "\n",
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# What the top predicted class is\n",
    "preds = model.predict(img)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam, heatmap = get_superimposed_img(img, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams, heatmaps = [], []\n",
    "\n",
    "for i in range(280,291):\n",
    "    \n",
    "    img = test_images[i]\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    preds = model.predict(img)\n",
    "    heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name)\n",
    "        \n",
    "    cam, heatmap = get_superimposed_img(img, heatmap)\n",
    "    \n",
    "    cams.append(cam)\n",
    "    heatmaps.append(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.axis('Off')\n",
    "    plt.subplots_adjust(wspace=0.1,hspace=0.05)\n",
    "    plt.imshow(cams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.axis('Off')\n",
    "    plt.subplots_adjust(wspace=0.1,hspace=0.05)\n",
    "    plt.imshow(heatmaps[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://keras.io/examples/vision/grad_cam/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2021-09-22",
   "language": "python",
   "name": "conda-2021-09-22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
